{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8a4834-80fc-459c-a14d-9aa7ec3b2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# === ETAPA 1: EXTRAÇÃO ===\n",
    "def carregar_dados(caminho):\n",
    "    tabelas = {\n",
    "        os.path.splitext(arquivo)[0]: pd.read_csv(os.path.join(caminho, arquivo), encoding='latin1')\n",
    "        for arquivo in os.listdir(caminho)\n",
    "        if arquivo.endswith('.csv')\n",
    "    }\n",
    "    return tabelas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b4b97-f9c4-4079-a733-4b1c4afdb45d",
   "metadata": {},
   "source": [
    "**Objetivo**:\n",
    "Carregar automaticamente todos os arquivos .csv de um diretório específico, convertendo cada um em um DataFrame e armazenando-os em um dicionário nomeado conforme o arquivo.\n",
    "\n",
    "**Detalhes técnicos**:\n",
    "1. **os.listdir()** lista os arquivos do diretório.\n",
    "2. **pd.read_csv()** lê cada CSV.\n",
    "3. **encoding='latin1'** garante a leitura correta de acentos e caracteres especiais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e0acff1-3f8c-4ab1-9c4f-fd58884ff160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPA 2: TRANSFORMAÇÃO ===\n",
    "def limpar_colunas(tabelas):\n",
    "    for df in tabelas.values():\n",
    "        df.columns = df.columns.str.strip()\n",
    "    return tabelas\n",
    "\n",
    "def transformar_dados(tabelas):\n",
    "    orders = tabelas['orders']\n",
    "    channels = tabelas['channels']\n",
    "    deliveries = tabelas['deliveries']\n",
    "    drivers = tabelas['drivers']\n",
    "    hubs = tabelas['hubs']\n",
    "    payments = tabelas['payments']\n",
    "    stores = tabelas['stores']\n",
    "\n",
    "    df = (\n",
    "        orders\n",
    "        .merge(channels, on='channel_id', how='left')\n",
    "        .merge(stores, on='store_id', how='left')\n",
    "        .merge(payments, left_on='order_id', right_on='payment_order_id', how='left')\n",
    "        .merge(deliveries, on='delivery_order_id', how='left')\n",
    "        .merge(drivers, on='driver_id', how='left')\n",
    "        .merge(hubs, on='hub_id', how='left')\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5146a4-511b-41ca-a452-7492c805188d",
   "metadata": {},
   "source": [
    "**Objetivo**:\n",
    "Remover espaços em branco dos nomes das colunas para padronização e evitar erros nas junções.\n",
    "\n",
    "**Detalhes técnicos**:\n",
    "1. **limpar_colunas**: Remove espaços extras nos nomes das colunas, garantindo consistência na hora de acessar ou unir os dados.\n",
    "2. **transformar_dados**: Realiza junções (merge) entre as principais tabelas do negócio, como pedidos, canais, entregas, motoristas, etc. O resultado é um DataFrame consolidado que representa uma visão integrada de cada pedido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367ffca8-0bd6-498c-9401-1b887394e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ETAPA 2.1: TRATAMENTOS AVANÇADOS ===\n",
    "\n",
    "def ajustar_tipos(df):\n",
    "    colunas_data = [col for col in df.columns if 'date' in col.lower() or 'data' in col.lower()]\n",
    "    for col in colunas_data:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def tratar_nulos(df):\n",
    "    for col in df.select_dtypes(include='number').columns:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col].fillna('desconhecido', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remover_duplicatas(df):\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "def padronizar_texto(df):\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].str.strip().str.lower().replace({'-': ' ', '_': ' '}, regex=True)\n",
    "    return df\n",
    "\n",
    "def remover_colunas_constantes(df):\n",
    "    return df.loc[:, df.nunique() > 1]\n",
    "\n",
    "def tratar_outliers_iqr(df):\n",
    "    for col in df.select_dtypes(include='number').columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        limite_inferior = Q1 - 1.5 * IQR\n",
    "        limite_superior = Q3 + 1.5 * IQR\n",
    "        df = df[(df[col] >= limite_inferior) & (df[col] <= limite_superior)]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e447a99c-90a9-4f80-93bc-c02ffcbe6bef",
   "metadata": {},
   "source": [
    "**Objetivo**:\n",
    "Essa etapa refina a base de dados para garantir que ela esteja limpa, padronizada e confiável. Isso reduz ruídos e viabiliza análises mais precisas, além de evitar que erros ou distorções afetem o desempenho de modelos e relatórios.\n",
    "\n",
    "**Detalhes técnicos**:\n",
    "**ajustar_tipos**: Converte colunas de datas e padroniza textos para minúsculo e sem espaços.\n",
    "**tratar_nulos**: Preenche valores ausentes com a mediana (numéricos) ou \"desconhecido\" (textuais).\n",
    "**remover_duplicatas**: Elimina registros repetidos.\n",
    "**padronizar_texto**: Remove caracteres inconsistentes e padroniza nomes.\n",
    "**remover_colunas_constantes**: Exclui colunas sem variação, que não agregam valor.\n",
    "**tratar_outliers_iqr**: Remove valores fora do intervalo interquartil (IQR), diminuindo o impacto de outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1904a37-f9a1-4fec-86a4-dbb7475d13d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ETL finalizado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# === ETAPA 3: CARGA ===\n",
    "def salvar_dados(df, caminho_saida):\n",
    "    df.to_csv(caminho_saida, index=False)\n",
    "\n",
    "# === EXECUÇÃO DO ETL ===\n",
    "if __name__ == '__main__':\n",
    "    caminho_entrada = '../data/raw'\n",
    "    caminho_saida = '../data/processed/dataset_final.csv'\n",
    "\n",
    "    tabelas = carregar_dados(caminho_entrada)\n",
    "    tabelas = limpar_colunas(tabelas)\n",
    "    df_final = transformar_dados(tabelas)\n",
    "    salvar_dados(df_final, caminho_saida)\n",
    "\n",
    "    print(\"✅ ETL finalizado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c0909e-26f5-4269-9d67-890e3221679a",
   "metadata": {},
   "source": [
    "**Objetivo**\n",
    "Salva o DataFrame final processado em um novo arquivo .csv, pronto para uso posterior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
